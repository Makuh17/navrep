{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "forced-resort",
   "metadata": {},
   "source": [
    "## Define ORCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "egyptian-eight",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from crowd_sim.envs.policy.orca import ORCA\n",
    "from crowd_sim.envs.utils.state import JointState\n",
    "import numpy as np\n",
    "\n",
    "    \n",
    "class ORCAPolicy(object):\n",
    "    def __init__(self, suicide_if_stuck=False):\n",
    "        self.simulator = ORCA()\n",
    "        self.suicide_if_stuck = suicide_if_stuck\n",
    "\n",
    "    def reset(self):\n",
    "        self.simulator.reset()\n",
    "\n",
    "    def predict(self, obs, env):\n",
    "        self.simulator.time_step = env._get_dt()\n",
    "        other_agent_states = [\n",
    "            agent.get_observable_state() for agent in env.soadrl_sim.humans + env.soadrl_sim.other_robots]\n",
    "        action = self.simulator.predict(\n",
    "            JointState(env.soadrl_sim.robot.get_full_state(), other_agent_states),\n",
    "            env.soadrl_sim.obstacle_vertices,\n",
    "            env.soadrl_sim.robot,\n",
    "        )\n",
    "        vx = action.v * np.cos(action.r)\n",
    "        vy = action.v * np.sin(action.r)\n",
    "        return np.array([vx, vy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-portfolio",
   "metadata": {},
   "source": [
    "## Setup Env and Dummy Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "spanish-anxiety",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ros was not found, disabled.\n"
     ]
    }
   ],
   "source": [
    "from navrep.envs.e2eenv import E2E1DNavRepEnv\n",
    "env = E2E1DNavRepEnv(silent=True, scenario='train', adaptive=False, collect_statistics=False)\n",
    "env.soadrl_sim.human_num = 2\n",
    "\n",
    "policy=ORCAPolicy(suicide_if_stuck=True)\n",
    "def policy_wrapper(_obs):\n",
    "    return policy.predict(_obs, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-synthetic",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unexpected-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines import DQN\n",
    "#from stable_baselines.gail import generate_expert_traj\n",
    "from crowd_sim.envs.policy.orca import ORCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "premium-quarter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from typing import Dict\n",
    "\n",
    "import cv2  # pytype:disable=import-error\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "\n",
    "from stable_baselines.common.base_class import BaseRLModel\n",
    "from stable_baselines.common.vec_env import VecEnv, VecFrameStack\n",
    "from stable_baselines.common.base_class import _UnvecWrapper\n",
    "\n",
    "\n",
    "def generate_expert_traj(model, save_path=None, env=None, n_timesteps=0,\n",
    "                         n_episodes=100, image_folder='recorded_images'):\n",
    "    \"\"\"\n",
    "    Train expert controller (if needed) and record expert trajectories.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        only Box and Discrete spaces are supported for now.\n",
    "\n",
    "    :param model: (RL model or callable) The expert model, if it needs to be trained,\n",
    "        then you need to pass ``n_timesteps > 0``.\n",
    "    :param save_path: (str) Path without the extension where the expert dataset will be saved\n",
    "        (ex: 'expert_cartpole' -> creates 'expert_cartpole.npz').\n",
    "        If not specified, it will not save, and just return the generated expert trajectories.\n",
    "        This parameter must be specified for image-based environments.\n",
    "    :param env: (gym.Env) The environment, if not defined then it tries to use the model\n",
    "        environment.\n",
    "    :param n_timesteps: (int) Number of training timesteps\n",
    "    :param n_episodes: (int) Number of trajectories (episodes) to record\n",
    "    :param image_folder: (str) When using images, folder that will be used to record images.\n",
    "    :return: (dict) the generated expert trajectories.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the environment using the RL model\n",
    "    if env is None and isinstance(model, BaseRLModel):\n",
    "        env = model.get_env()\n",
    "\n",
    "    assert env is not None, \"You must set the env in the model or pass it to the function.\"\n",
    "\n",
    "    is_vec_env = False\n",
    "    if isinstance(env, VecEnv) and not isinstance(env, _UnvecWrapper):\n",
    "        is_vec_env = True\n",
    "        if env.num_envs > 1:\n",
    "            warnings.warn(\"You are using multiple envs, only the data from the first one will be recorded.\")\n",
    "\n",
    "    # Sanity check\n",
    "    assert (isinstance(env.observation_space, spaces.Box) or\n",
    "            isinstance(env.observation_space, spaces.Discrete)), \"Observation space type not supported\"\n",
    "\n",
    "    assert (isinstance(env.action_space, spaces.Box) or\n",
    "            isinstance(env.action_space, spaces.Discrete)), \"Action space type not supported\"\n",
    "\n",
    "    # Check if we need to record images\n",
    "    obs_space = env.observation_space\n",
    "    record_images = len(obs_space.shape) == 3 and obs_space.shape[-1] in [1, 3, 4] \\\n",
    "                    and obs_space.dtype == np.uint8\n",
    "    if record_images and save_path is None:\n",
    "        warnings.warn(\"Observations are images but no save path was specified, so will save in numpy archive; \"\n",
    "                      \"this can lead to higher memory usage.\")\n",
    "        record_images = False\n",
    "\n",
    "    if not record_images and len(obs_space.shape) == 3 and obs_space.dtype == np.uint8:\n",
    "        warnings.warn(\"The observations looks like images (shape = {}) \"\n",
    "                      \"but the number of channel > 4, so it will be saved in the numpy archive \"\n",
    "                      \"which can lead to high memory usage\".format(obs_space.shape))\n",
    "\n",
    "    image_ext = 'jpg'\n",
    "    if record_images:\n",
    "        # We save images as jpg or png, that have only 3/4 color channels\n",
    "        if isinstance(env, VecFrameStack) and env.n_stack == 4:\n",
    "            # assert env.n_stack < 5, \"The current data recorder does no support\"\\\n",
    "            #                          \"VecFrameStack with n_stack > 4\"\n",
    "            image_ext = 'png'\n",
    "\n",
    "        folder_path = os.path.dirname(save_path)\n",
    "        image_folder = os.path.join(folder_path, image_folder)\n",
    "        os.makedirs(image_folder, exist_ok=True)\n",
    "        print(\"=\" * 10)\n",
    "        print(\"Images will be recorded to {}/\".format(image_folder))\n",
    "        print(\"Image shape: {}\".format(obs_space.shape))\n",
    "        print(\"=\" * 10)\n",
    "\n",
    "    if n_timesteps > 0 and isinstance(model, BaseRLModel):\n",
    "        model.learn(n_timesteps)\n",
    "\n",
    "    actions = []\n",
    "    observations = []\n",
    "    rewards = []\n",
    "    episode_returns = np.zeros((n_episodes,))\n",
    "    episode_starts = []\n",
    "\n",
    "    ep_idx = 0\n",
    "    obs = env.reset()\n",
    "    episode_starts.append(True)\n",
    "    reward_sum = 0.0\n",
    "    idx = 0\n",
    "    # state and mask for recurrent policies\n",
    "    state, mask = None, None\n",
    "\n",
    "    if is_vec_env:\n",
    "        mask = [True for _ in range(env.num_envs)]\n",
    "\n",
    "    while ep_idx < n_episodes:\n",
    "        obs_ = obs[0] if is_vec_env else obs\n",
    "        if record_images:\n",
    "            image_path = os.path.join(image_folder, \"{}.{}\".format(idx, image_ext))\n",
    "            # Convert from RGB to BGR\n",
    "            # which is the format OpenCV expect\n",
    "            if obs_.shape[-1] == 3:\n",
    "                obs_ = cv2.cvtColor(obs_, cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite(image_path, obs_)\n",
    "            observations.append(image_path)\n",
    "        else:\n",
    "            observations.append(obs_)\n",
    "\n",
    "        if isinstance(model, BaseRLModel):\n",
    "            action, state = model.predict(obs, state=state, mask=mask)\n",
    "        else:\n",
    "            action = model(obs)\n",
    "\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Use only first env\n",
    "        if is_vec_env:\n",
    "            mask = [done[0] for _ in range(env.num_envs)]\n",
    "            action = np.array([action[0]])\n",
    "            reward = np.array([reward[0]])\n",
    "            done = np.array([done[0]])\n",
    "\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        episode_starts.append(done)\n",
    "        reward_sum += reward\n",
    "        idx += 1\n",
    "        if done:\n",
    "            if not is_vec_env:\n",
    "                obs = env.reset()\n",
    "                # Reset the state in case of a recurrent policy\n",
    "                state = None\n",
    "\n",
    "            episode_returns[ep_idx] = reward_sum\n",
    "            reward_sum = 0.0\n",
    "            ep_idx += 1\n",
    "\n",
    "    if isinstance(env.observation_space, spaces.Box) and not record_images:\n",
    "        observations = np.concatenate(observations).reshape((-1,) + env.observation_space.shape)\n",
    "    elif isinstance(env.observation_space, spaces.Discrete):\n",
    "        observations = np.array(observations).reshape((-1, 1))\n",
    "    elif record_images:\n",
    "        observations = np.array(observations)\n",
    "\n",
    "    if isinstance(env.action_space, spaces.Box):\n",
    "        actions = np.concatenate(actions).reshape((-1,) + env.action_space.shape)\n",
    "    elif isinstance(env.action_space, spaces.Discrete):\n",
    "        actions = np.array(actions).reshape((-1, 1))\n",
    "\n",
    "    rewards = np.array(rewards)\n",
    "    episode_starts = np.array(episode_starts[:-1])\n",
    "\n",
    "    assert len(observations) == len(actions)\n",
    "\n",
    "    numpy_dict = {\n",
    "        'actions': actions,\n",
    "        'obs': observations,\n",
    "        'rewards': rewards,\n",
    "        'episode_returns': episode_returns,\n",
    "        'episode_starts': episode_starts\n",
    "    }  # type: Dict[str, np.ndarray]\n",
    "\n",
    "    for key, val in numpy_dict.items():\n",
    "        print(key, val.shape)\n",
    "\n",
    "    if save_path is not None:\n",
    "        np.savez(save_path, **numpy_dict)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return numpy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "undefined-wyoming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions (1590, 2)\n",
      "obs (1590, 1085)\n",
      "rewards (1590,)\n",
      "episode_returns (10,)\n",
      "episode_starts (1590,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'actions': array([[-1.09251620e-01,  1.80274961e-01],\n",
       "        [-1.04881582e-01,  1.73064005e-01],\n",
       "        [-1.00686299e-01,  1.66141413e-01],\n",
       "        ...,\n",
       "        [-2.33337885e-04, -6.86663597e-05],\n",
       "        [-2.24003226e-04, -6.59193688e-05],\n",
       "        [-2.15040238e-04, -6.32817527e-05]]),\n",
       " 'obs': array([[ 1.92945540e+00,  1.94818461e+00,  1.96734834e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 2.01079941e+00,  2.03031826e+00,  2.05028987e+00, ...,\n",
       "         -1.09251620e-01,  1.80274961e-01,  0.00000000e+00],\n",
       "        [ 2.08889008e+00,  2.10916710e+00,  2.12991428e+00, ...,\n",
       "         -1.04881582e-01,  1.73064005e-01,  0.00000000e+00],\n",
       "        ...,\n",
       "        [ 3.55879784e+00,  3.55277753e+00,  3.54689693e+00, ...,\n",
       "         -2.43029920e-04, -7.15185188e-05,  0.00000000e+00],\n",
       "        [ 3.55884862e+00,  3.55282807e+00,  3.54694748e+00, ...,\n",
       "         -2.33337885e-04, -6.86663597e-05,  0.00000000e+00],\n",
       "        [ 3.55889726e+00,  3.55287671e+00,  3.54699588e+00, ...,\n",
       "         -2.24003226e-04, -6.59193688e-05,  0.00000000e+00]]),\n",
       " 'rewards': array([4.21592116e-03, 4.04728532e-03, 3.88539314e-03, ...,\n",
       "        4.86463308e-06, 4.67002392e-06, 0.00000000e+00]),\n",
       " 'episode_returns': array([-1.20894685e+02,  2.93590918e-02, -1.74813249e+02, -1.26629766e+01,\n",
       "         3.28468391e-01, -1.40935650e+02, -2.59287110e+01,  3.28858594e-01,\n",
       "         3.29806984e-01,  1.41821840e-01]),\n",
       " 'episode_starts': array([ True, False, False, ..., False, False, False])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = DQN('MlpPolicy', 'CartPole-v1', verbose=1)\n",
    "      # Train a DQN agent for 1e5 timesteps and generate 10 trajectories\n",
    "      # data will be saved in a numpy archive named `expert_cartpole.npz`\n",
    "#generate_expert_traj(model, 'expert_cartpole', n_timesteps=int(1e5), n_episodes=10)\n",
    "\n",
    "generate_expert_traj(policy_wrapper, 'orca_1', env, n_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-mexican",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, reward, done, _ = env.step([0,0])\n",
    "print(obs[-15:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-behavior",
   "metadata": {},
   "source": [
    "## Generate Trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-roots",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rocky-heater",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions (1590, 2)\n",
      "obs (1590, 1085)\n",
      "rewards (1590,)\n",
      "episode_returns (10,)\n",
      "episode_starts (1590,)\n",
      "Total trajectories: 1\n",
      "Total transitions: 350\n",
      "Average returns: -47.40769568513271\n",
      "Std for returns: 65.8757565688237\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "WARNING:tensorflow:From /home/mads/git/navrep/navrep/tools/custom_policy.py:26: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv1d instead.\n",
      "WARNING:tensorflow:From /home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/mads/git/navrep/navrep/tools/custom_policy.py:38: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Pretraining with Behavior Cloning...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (64, 1085) for Tensor 'input/Ob:0', which has shape '(?, 1085, 1)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8996465be8f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCustom1DPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Pretrain the PPO2 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# As an option, you can train the RL agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/stable_baselines/common/base_class.py\u001b[0m in \u001b[0;36mpretrain\u001b[0;34m(self, dataset, n_epochs, learning_rate, adam_epsilon, val_interval)\u001b[0m\n\u001b[1;32m    349\u001b[0m                     \u001b[0mactions_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexpert_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 }\n\u001b[0;32m--> 351\u001b[0;31m                 \u001b[0mtrain_loss_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_loss_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1128\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1129\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (64, 1085) for Tensor 'input/Ob:0', which has shape '(?, 1085, 1)'"
     ]
    }
   ],
   "source": [
    "from stable_baselines import PPO2\n",
    "from stable_baselines.gail import ExpertDataset\n",
    "from navrep.tools.custom_policy import Custom1DPolicy, ARCH, _C\n",
    "# Using only one expert trajectory\n",
    "# you can specify `traj_limitation=-1` for using the whole dataset\n",
    "dataset = ExpertDataset(expert_path='orca_1.npz',\n",
    "                        traj_limitation=1, batch_size=64)\n",
    "\n",
    "model = PPO2(Custom1DPolicy, env, verbose=1)\n",
    "# Pretrain the PPO2 model\n",
    "model.pretrain(dataset, n_epochs=1000)\n",
    "\n",
    "# As an option, you can train the RL agent\n",
    "# model.learn(int(1e5))\n",
    "\n",
    "# Test the pre-trained model\n",
    "env = model.get_env()\n",
    "obs = env.reset()\n",
    "\n",
    "reward_sum = 0.0\n",
    "for _ in range(1000):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "        env.render()\n",
    "        if done:\n",
    "                print(reward_sum)\n",
    "                reward_sum = 0.0\n",
    "                obs = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/mads/navrep/datasets/V/navreptrain/099_scans_robotstates_actions_rewards_dones.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.load(\"expert_cartpole.npz\")\n",
    "for k in tmp.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.load(path)\n",
    "for k in tmp.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-istanbul",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
