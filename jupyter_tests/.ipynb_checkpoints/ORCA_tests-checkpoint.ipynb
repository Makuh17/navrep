{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cosmetic-mileage",
   "metadata": {},
   "source": [
    "## ORCA Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "continued-sound",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from crowd_sim.envs.policy.orca import ORCA\n",
    "from crowd_sim.envs.utils.state import JointState\n",
    "\n",
    "class Suicide(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "class ORCAPolicy(object):\n",
    "    def __init__(self, suicide_if_stuck=False):\n",
    "        self.simulator = ORCA()\n",
    "        self.suicide_if_stuck = suicide_if_stuck\n",
    "\n",
    "    def reset(self):\n",
    "        self.simulator.reset()\n",
    "\n",
    "    def predict(self, obs, env):\n",
    "        self.simulator.time_step = env._get_dt()\n",
    "        other_agent_states = [\n",
    "            agent.get_observable_state() for agent in env.soadrl_sim.humans + env.soadrl_sim.other_robots]\n",
    "        \n",
    "        \n",
    "        action = self.simulator.predict(\n",
    "            JointState(env.soadrl_sim.robot.get_full_state(), other_agent_states),\n",
    "            env.soadrl_sim.obstacle_vertices,\n",
    "            env.soadrl_sim.robot,\n",
    "        )\n",
    "        \n",
    "        if self.suicide_if_stuck:\n",
    "            if action.v < 0.1:\n",
    "                return Suicide()\n",
    "        vx = action.v * np.cos(action.r)\n",
    "        vy = action.v * np.sin(action.r)\n",
    "        return np.array([vx, vy, 0.1*(np.random.random()-0.5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-pottery",
   "metadata": {},
   "source": [
    "## Play Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "concrete-roommate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def play_policy(env, n_sequences, episode_length=1000,\n",
    "                         subset_index=0, n_subsets=1,\n",
    "                         render=True,\n",
    "                         policy=ORCAPolicy(),\n",
    "                         archive_dir=os.path.expanduser(\"~/navrep/datasets/V/ian\")\n",
    "                         ):\n",
    "    \"\"\"\n",
    "    if n_subsets is None, the whole set of sequences is generated (n_sequences)\n",
    "    if n_subsets is a number > 1, this function only generates a portion of the sequences\n",
    "    \"\"\"\n",
    "    indices = np.arange(n_sequences)\n",
    "    if n_subsets > 1:  # when multiprocessing\n",
    "        indices = np.array_split(indices, n_subsets)[subset_index]\n",
    "    for n in indices:\n",
    "        scans = []\n",
    "        robotstates = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "        policy.reset()\n",
    "        obs = env.reset()\n",
    "        for i in range(episode_length):\n",
    "            # step\n",
    "            action = policy.predict(obs, env)\n",
    "            if isinstance(action, Suicide):\n",
    "                obs = env.reset()\n",
    "                rew = 0\n",
    "                action = np.array([0, 0, 0])\n",
    "                done = True\n",
    "            else:\n",
    "                obs, rew, done, _ = env.step(action)\n",
    "            scans.append(obs[0])\n",
    "            robotstates.append(obs[1])\n",
    "            actions.append(action)\n",
    "            rewards.append(rew)\n",
    "            dones.append(done)\n",
    "            if render:\n",
    "                env.render()\n",
    "            if done:\n",
    "                policy.reset()\n",
    "                obs = env.reset()\n",
    "        dones[-1] = True\n",
    "\n",
    "        scans = np.array(scans)\n",
    "        robotstates = np.array(robotstates)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards)\n",
    "        dones = np.array(dones)\n",
    "        data = dict(scans=scans, robotstates=robotstates, actions=actions, rewards=rewards, dones=dones)\n",
    "        #if archive_dir is not None:\n",
    "         #   make_dir_if_not_exists(archive_dir)\n",
    "          #  archive_path = os.path.join(\n",
    "           #     archive_dir, \"{:03}_scans_robotstates_actions_rewards_dones.npz\".format(n)\n",
    "            #)\n",
    "            #np.savez_compressed(archive_path, **data)\n",
    "            #print(archive_path, \"written.\")\n",
    "    env.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "extensive-crash",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ros was not found, disabled.\n",
      "[(2.840652011665128, 4.340652011665128), (1.5593479883348724, 4.340652011665128), (1.5593479883348724, 3.0593479883348724), (2.840652011665128, 3.0593479883348724)]\n",
      "[(2.5797941196990735, 6.779794119699074), (0.6202058803009269, 6.779794119699074), (0.6202058803009269, 4.820205880300928), (2.5797941196990735, 4.820205880300928)]\n",
      "[(0.6730411372675034, 7.473041137267503), (-0.4730411372675034, 7.473041137267503), (-0.4730411372675034, 6.326958862732497), (0.6730411372675034, 6.326958862732497)]\n",
      "[(-3.875, -4.95), (-5.125, -4.95), (-5.125, -7.45), (-3.875, -7.45)]\n",
      "[(-4.925000000000001, 7.025), (-8.675, 7.025), (-8.675, 5.775), (-4.925000000000001, 5.775)]\n",
      "[(-4.575, 7.3500000000000005), (-5.825, 7.3500000000000005), (-5.825, 4.8500000000000005), (-4.575, 4.8500000000000005)]\n",
      "[(3.25, 6.525), (0.75, 6.525), (0.75, 5.275), (3.25, 5.275)]\n",
      "[(-4.75, -0.17500000000000004), (-7.25, -0.17500000000000004), (-7.25, -1.425), (-4.75, -1.425)]\n",
      "[(-0.42500000000000027, 6.625), (-4.175000000000001, 6.625), (-4.175000000000001, 5.375), (-0.42500000000000027, 5.375)]\n",
      "[(-9.675, 8.473041137267504), (6.9472737056254745, 8.473041137267504), (6.9472737056254745, -8.45), (-9.675, -8.45)]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'msg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4b9e4119fcf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mE2E1DNavRepEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscenario\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madaptive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollect_statistics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoadrl_sim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhuman_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplay_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-12edbce395eb>\u001b[0m in \u001b[0;36mplay_policy\u001b[0;34m(env, n_sequences, episode_length, subset_index, n_subsets, render, policy, archive_dir)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSuicide\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-26371f8ef40b>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, obs, env)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvertex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoadrl_sim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobstacle_vertices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvertex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         action = self.simulator.predict(\n\u001b[1;32m     26\u001b[0m             \u001b[0mJointState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoadrl_sim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_full_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_agent_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'msg' is not defined"
     ]
    }
   ],
   "source": [
    "from navrep.envs.e2eenv import E2E1DNavRepEnv\n",
    "env = E2E1DNavRepEnv(silent=True, scenario='train', adaptive=False, collect_statistics=False)\n",
    "env.soadrl_sim.human_num = 2\n",
    "play_policy(env,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-adams",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
