{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "federal-onion",
   "metadata": {},
   "source": [
    "## Define ORCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "answering-triumph",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crowd_sim.envs.policy.orca import ORCA\n",
    "from crowd_sim.envs.utils.state import JointState\n",
    "import numpy as np\n",
    "\n",
    "    \n",
    "class ORCAPolicy(object):\n",
    "    def __init__(self, suicide_if_stuck=False):\n",
    "        self.simulator = ORCA()\n",
    "        self.suicide_if_stuck = suicide_if_stuck\n",
    "\n",
    "    def reset(self):\n",
    "        self.simulator.reset()\n",
    "\n",
    "    def predict(self, obs, env):\n",
    "        self.simulator.time_step = env._get_dt()\n",
    "        other_agent_states = [\n",
    "            agent.get_observable_state() for agent in env.soadrl_sim.humans + env.soadrl_sim.other_robots]\n",
    "        action = self.simulator.predict(\n",
    "            JointState(env.soadrl_sim.robot.get_full_state(), other_agent_states),\n",
    "            env.soadrl_sim.obstacle_vertices,\n",
    "            env.soadrl_sim.robot,\n",
    "        )\n",
    "        vx = action.v * np.cos(action.r)\n",
    "        vy = action.v * np.sin(action.r)\n",
    "        return np.array([vx, vy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accomplished-atlantic",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ba76bd8fc719>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mreward_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mreward_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-ba76bd8fc719>\u001b[0m in \u001b[0;36mpolicy_wrapper\u001b[0;34m(_obs)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpolicy_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mE2E1DNavRepEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscenario\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madaptive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollect_statistics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'policy' is not defined"
     ]
    }
   ],
   "source": [
    "# Test the pre-trained model\n",
    "from navrep.envs.e2eenv import E2E1DNavRepEnv, E2ENavRepEnv\n",
    "\n",
    "def policy_wrapper(_obs):\n",
    "    return policy.predict(_obs, env)\n",
    "\n",
    "env = E2E1DNavRepEnv(silent=True, scenario='train', adaptive=False, collect_statistics=False)\n",
    "env.soadrl_sim.human_num = 2\n",
    "obs = env.reset()\n",
    "model = policy_wrapper\n",
    "\n",
    "reward_sum = 0.0\n",
    "for _ in range(1000):\n",
    "        action = model(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "        env.render()\n",
    "        if done:\n",
    "                print(reward_sum)\n",
    "                reward_sum = 0.0\n",
    "                obs = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-medicine",
   "metadata": {},
   "source": [
    "## Setup Env and Dummy Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "worthy-understanding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ros was not found, disabled.\n"
     ]
    }
   ],
   "source": [
    "from navrep.envs.e2eenv import E2E1DNavRepEnv, E2ENavRepEnv\n",
    "env = E2E1DNavRepEnv(silent=True, scenario='train', adaptive=False, collect_statistics=False)\n",
    "env.soadrl_sim.human_num = 2\n",
    "\n",
    "policy=ORCAPolicy(suicide_if_stuck=True)\n",
    "def policy_wrapper(_obs):\n",
    "    return policy.predict(_obs, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-honduras",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stuffed-visiting",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines import DQN\n",
    "from stable_baselines.gail import generate_expert_traj\n",
    "from crowd_sim.envs.policy.orca import ORCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-google",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from typing import Dict\n",
    "\n",
    "import cv2  # pytype:disable=import-error\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "\n",
    "from stable_baselines.common.base_class import BaseRLModel\n",
    "from stable_baselines.common.vec_env import VecEnv, VecFrameStack\n",
    "from stable_baselines.common.base_class import _UnvecWrapper\n",
    "\n",
    "\n",
    "def generate_expert_traj(model, save_path=None, env=None, n_timesteps=0,\n",
    "                         n_episodes=100, image_folder='recorded_images'):\n",
    "    \"\"\"\n",
    "    Train expert controller (if needed) and record expert trajectories.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        only Box and Discrete spaces are supported for now.\n",
    "\n",
    "    :param model: (RL model or callable) The expert model, if it needs to be trained,\n",
    "        then you need to pass ``n_timesteps > 0``.\n",
    "    :param save_path: (str) Path without the extension where the expert dataset will be saved\n",
    "        (ex: 'expert_cartpole' -> creates 'expert_cartpole.npz').\n",
    "        If not specified, it will not save, and just return the generated expert trajectories.\n",
    "        This parameter must be specified for image-based environments.\n",
    "    :param env: (gym.Env) The environment, if not defined then it tries to use the model\n",
    "        environment.\n",
    "    :param n_timesteps: (int) Number of training timesteps\n",
    "    :param n_episodes: (int) Number of trajectories (episodes) to record\n",
    "    :param image_folder: (str) When using images, folder that will be used to record images.\n",
    "    :return: (dict) the generated expert trajectories.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the environment using the RL model\n",
    "    if env is None and isinstance(model, BaseRLModel):\n",
    "        env = model.get_env()\n",
    "\n",
    "    assert env is not None, \"You must set the env in the model or pass it to the function.\"\n",
    "\n",
    "    is_vec_env = False\n",
    "    if isinstance(env, VecEnv) and not isinstance(env, _UnvecWrapper):\n",
    "        is_vec_env = True\n",
    "        if env.num_envs > 1:\n",
    "            warnings.warn(\"You are using multiple envs, only the data from the first one will be recorded.\")\n",
    "\n",
    "    # Sanity check\n",
    "    assert (isinstance(env.observation_space, spaces.Box) or\n",
    "            isinstance(env.observation_space, spaces.Discrete)), \"Observation space type not supported\"\n",
    "\n",
    "    assert (isinstance(env.action_space, spaces.Box) or\n",
    "            isinstance(env.action_space, spaces.Discrete)), \"Action space type not supported\"\n",
    "\n",
    "    # Check if we need to record images\n",
    "    obs_space = env.observation_space\n",
    "    record_images = len(obs_space.shape) == 3 and obs_space.shape[-1] in [1, 3, 4] \\\n",
    "                    and obs_space.dtype == np.uint8\n",
    "    if record_images and save_path is None:\n",
    "        warnings.warn(\"Observations are images but no save path was specified, so will save in numpy archive; \"\n",
    "                      \"this can lead to higher memory usage.\")\n",
    "        record_images = False\n",
    "\n",
    "    if not record_images and len(obs_space.shape) == 3 and obs_space.dtype == np.uint8:\n",
    "        warnings.warn(\"The observations looks like images (shape = {}) \"\n",
    "                      \"but the number of channel > 4, so it will be saved in the numpy archive \"\n",
    "                      \"which can lead to high memory usage\".format(obs_space.shape))\n",
    "\n",
    "    image_ext = 'jpg'\n",
    "    if record_images:\n",
    "        # We save images as jpg or png, that have only 3/4 color channels\n",
    "        if isinstance(env, VecFrameStack) and env.n_stack == 4:\n",
    "            # assert env.n_stack < 5, \"The current data recorder does no support\"\\\n",
    "            #                          \"VecFrameStack with n_stack > 4\"\n",
    "            image_ext = 'png'\n",
    "\n",
    "        folder_path = os.path.dirname(save_path)\n",
    "        image_folder = os.path.join(folder_path, image_folder)\n",
    "        os.makedirs(image_folder, exist_ok=True)\n",
    "        print(\"=\" * 10)\n",
    "        print(\"Images will be recorded to {}/\".format(image_folder))\n",
    "        print(\"Image shape: {}\".format(obs_space.shape))\n",
    "        print(\"=\" * 10)\n",
    "\n",
    "    if n_timesteps > 0 and isinstance(model, BaseRLModel):\n",
    "        model.learn(n_timesteps)\n",
    "\n",
    "    actions = []\n",
    "    observations = []\n",
    "    rewards = []\n",
    "    episode_returns = np.zeros((n_episodes,))\n",
    "    episode_starts = []\n",
    "\n",
    "    ep_idx = 0\n",
    "    obs = env.reset()\n",
    "    episode_starts.append(True)\n",
    "    reward_sum = 0.0\n",
    "    idx = 0\n",
    "    # state and mask for recurrent policies\n",
    "    state, mask = None, None\n",
    "\n",
    "    if is_vec_env:\n",
    "        mask = [True for _ in range(env.num_envs)]\n",
    "\n",
    "    while ep_idx < n_episodes:\n",
    "        obs_ = obs[0] if is_vec_env else obs\n",
    "        if record_images:\n",
    "            image_path = os.path.join(image_folder, \"{}.{}\".format(idx, image_ext))\n",
    "            # Convert from RGB to BGR\n",
    "            # which is the format OpenCV expect\n",
    "            if obs_.shape[-1] == 3:\n",
    "                obs_ = cv2.cvtColor(obs_, cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite(image_path, obs_)\n",
    "            observations.append(image_path)\n",
    "        else:\n",
    "            observations.append(obs_)\n",
    "\n",
    "        if isinstance(model, BaseRLModel):\n",
    "            action, state = model.predict(obs, state=state, mask=mask)\n",
    "        else:\n",
    "            action = model(obs)\n",
    "\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Use only first env\n",
    "        if is_vec_env:\n",
    "            mask = [done[0] for _ in range(env.num_envs)]\n",
    "            action = np.array([action[0]])\n",
    "            reward = np.array([reward[0]])\n",
    "            done = np.array([done[0]])\n",
    "\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        episode_starts.append(done)\n",
    "        reward_sum += reward\n",
    "        idx += 1\n",
    "        if done:\n",
    "            if not is_vec_env:\n",
    "                obs = env.reset()\n",
    "                # Reset the state in case of a recurrent policy\n",
    "                state = None\n",
    "\n",
    "            episode_returns[ep_idx] = reward_sum\n",
    "            reward_sum = 0.0\n",
    "            ep_idx += 1\n",
    "\n",
    "    if isinstance(env.observation_space, spaces.Box) and not record_images:\n",
    "        observations = np.concatenate(observations).reshape((-1,) + env.observation_space.shape)\n",
    "    elif isinstance(env.observation_space, spaces.Discrete):\n",
    "        observations = np.array(observations).reshape((-1, 1))\n",
    "    elif record_images:\n",
    "        observations = np.array(observations)\n",
    "\n",
    "    if isinstance(env.action_space, spaces.Box):\n",
    "        actions = np.concatenate(actions).reshape((-1,) + env.action_space.shape)\n",
    "    elif isinstance(env.action_space, spaces.Discrete):\n",
    "        actions = np.array(actions).reshape((-1, 1))\n",
    "\n",
    "    rewards = np.array(rewards)\n",
    "    episode_starts = np.array(episode_starts[:-1])\n",
    "\n",
    "    assert len(observations) == len(actions)\n",
    "\n",
    "    numpy_dict = {\n",
    "        'actions': actions,\n",
    "        'obs': observations,\n",
    "        'rewards': rewards,\n",
    "        'episode_returns': episode_returns,\n",
    "        'episode_starts': episode_starts\n",
    "    }  # type: Dict[str, np.ndarray]\n",
    "\n",
    "    for key, val in numpy_dict.items():\n",
    "        print(key, val.shape)\n",
    "\n",
    "    if save_path is not None:\n",
    "        np.savez(save_path, **numpy_dict)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return numpy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "polished-tackle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions (1029, 2)\n",
      "obs (1029, 1085, 1)\n",
      "rewards (1029,)\n",
      "episode_returns (10,)\n",
      "episode_starts (1029,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'actions': array([[ 0.46623183, -0.78995897],\n",
       "        [ 0.49181318, -0.87070078],\n",
       "        [ 0.49181318, -0.87070078],\n",
       "        ...,\n",
       "        [-0.01184773, -0.07874081],\n",
       "        [-0.01137818, -0.07559887],\n",
       "        [-0.01092706, -0.07258199]]),\n",
       " 'obs': array([[[11.00585365],\n",
       "         [10.96998501],\n",
       "         [10.93471622],\n",
       "         ...,\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       " \n",
       "        [[ 4.52616596],\n",
       "         [ 4.5733099 ],\n",
       "         [ 4.62160397],\n",
       "         ...,\n",
       "         [ 0.46623183],\n",
       "         [-0.78995897],\n",
       "         [ 0.        ]],\n",
       " \n",
       "        [[ 4.11965609],\n",
       "         [ 4.16256571],\n",
       "         [ 4.20652246],\n",
       "         ...,\n",
       "         [ 0.49181318],\n",
       "         [-0.87070078],\n",
       "         [ 0.        ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 8.84990692],\n",
       "         [ 8.84260178],\n",
       "         [ 8.83560753],\n",
       "         ...,\n",
       "         [-0.01233644],\n",
       "         [-0.08201295],\n",
       "         [ 0.        ]],\n",
       " \n",
       "        [[ 8.85455894],\n",
       "         [ 8.84724998],\n",
       "         [ 8.84025192],\n",
       "         ...,\n",
       "         [-0.01184773],\n",
       "         [-0.07874081],\n",
       "         [ 0.        ]],\n",
       " \n",
       "        [[ 8.85902691],\n",
       "         [ 8.85171413],\n",
       "         [ 8.84471226],\n",
       "         ...,\n",
       "         [-0.01137818],\n",
       "         [-0.07559887],\n",
       "         [ 0.        ]]]),\n",
       " 'rewards': array([ 1.83423724e-02,  2.00000004e-02,  2.00000004e-02, ...,\n",
       "        -9.98510811e-01, -9.98570697e-01, -2.50000000e+01]),\n",
       " 'episode_returns': array([-59.80953911,   0.88877198, -26.93714301, -25.81907563,\n",
       "          0.37775582, -26.82894065,   0.89069665,   0.56451333,\n",
       "        -46.90150035, -50.971888  ]),\n",
       " 'episode_starts': array([ True, False, False, ..., False, False, False])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = DQN('MlpPolicy', 'CartPole-v1', verbose=1)\n",
    "      # Train a DQN agent for 1e5 timesteps and generate 10 trajectories\n",
    "      # data will be saved in a numpy archive named `expert_cartpole.npz`\n",
    "#generate_expert_traj(model, 'expert_cartpole', n_timesteps=int(1e5), n_episodes=10)\n",
    "\n",
    "generate_expert_traj(policy_wrapper, 'orca_2', env, n_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "rocky-synthetic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.87815832  79.3231428   96.37074618  95.95627656 100.80249118\n",
      "  68.06827859 101.3948617   86.47807654  88.92643232  86.04454543\n",
      "  81.46278628  88.42050718  77.2356548   47.6604951   95.01580569\n",
      " 101.29317304 101.11220076  85.66853445  92.79870004 101.2258932\n",
      "  95.34440513  95.78825557 100.97979292  89.70319052  87.92813825\n",
      "  85.1491326  101.32853597  70.37319616  86.6744627  100.96739427\n",
      "  95.26622961 101.14252187  94.83092779  90.71089258 100.7770334\n",
      "  84.06563106  74.44746598  89.82597272  87.59845031 101.00386511\n",
      "  83.86101075]\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "tmp = np.load('orca_1.npz')\n",
    "print(tmp['episode_returns'][tmp['episode_returns']>30])\n",
    "print(tmp['episode_returns'].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-enough",
   "metadata": {},
   "source": [
    "## Generate Trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-wallet",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "organic-network",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ros was not found, disabled.\n",
      "actions (102652, 2)\n",
      "obs (102652, 4101, 1)\n",
      "rewards (102652,)\n",
      "episode_returns (1000,)\n",
      "episode_starts (102652,)\n",
      "Total trajectories: 1\n",
      "Total transitions: 216\n",
      "Average returns: -16.26382005844153\n",
      "Std for returns: 38.4109453780394\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "WARNING:tensorflow:From /home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Pretraining with Behavior Cloning...\n",
      "==== Training progress 10.00% ====\n",
      "Epoch 500\n",
      "Training loss: 0.000000, Validation loss: 0.000006\n",
      "\n",
      "==== Training progress 20.00% ====\n",
      "Epoch 1000\n",
      "Training loss: 0.000000, Validation loss: 0.000006\n",
      "\n",
      "==== Training progress 30.00% ====\n",
      "Epoch 1500\n",
      "Training loss: 0.000000, Validation loss: 0.000005\n",
      "\n",
      "==== Training progress 40.00% ====\n",
      "Epoch 2000\n",
      "Training loss: 0.000000, Validation loss: 0.000007\n",
      "\n",
      "==== Training progress 50.00% ====\n",
      "Epoch 2500\n",
      "Training loss: 0.000000, Validation loss: 0.000028\n",
      "\n",
      "==== Training progress 60.00% ====\n",
      "Epoch 3000\n",
      "Training loss: 0.000000, Validation loss: 0.000009\n",
      "\n",
      "==== Training progress 70.00% ====\n",
      "Epoch 3500\n",
      "Training loss: 0.000000, Validation loss: 0.000004\n",
      "\n",
      "==== Training progress 80.00% ====\n",
      "Epoch 4000\n",
      "Training loss: 0.000001, Validation loss: 0.000029\n",
      "\n",
      "==== Training progress 90.00% ====\n",
      "Epoch 4500\n",
      "Training loss: 0.000000, Validation loss: 0.000014\n",
      "\n",
      "==== Training progress 100.00% ====\n",
      "Epoch 5000\n",
      "Training loss: 0.000000, Validation loss: 0.000005\n",
      "\n",
      "Pretraining done.\n",
      "[-25.052582]\n",
      "[-25.213562]\n",
      "[-25.057182]\n",
      "[-25.057549]\n",
      "[-33.125484]\n",
      "[-25.177303]\n",
      "[-30.71418]\n",
      "[-26.2781]\n",
      "[-25.077862]\n",
      "[-25.423328]\n",
      "[-25.107975]\n",
      "[-42.50477]\n",
      "[-27.719006]\n",
      "[-25.067245]\n",
      "[-25.029238]\n",
      "[-26.65806]\n",
      "[-25.055534]\n",
      "[-25.034792]\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines import PPO2\n",
    "from stable_baselines.gail import ExpertDataset\n",
    "from navrep.tools.custom_policy import CustomPolicy, Custom1DPolicy, ARCH, _C\n",
    "from navrep.envs.e2eenv import E2E1DNavRepEnv, E2ENavRepEnv\n",
    "env = E2ENavRepEnv(silent=True, scenario='train', adaptive=False, collect_statistics=False)\n",
    "# Using only one expert trajectory\n",
    "# you can specify `traj_limitation=-1` for using the whole dataset\n",
    "dataset = ExpertDataset(expert_path='orca_1.npz',\n",
    "                        traj_limitation=1, batch_size=64)\n",
    "\n",
    "model = PPO2('MlpPolicy', env, verbose=1)\n",
    "#model = PPO2(CustomPolicy, env, verbose=1)\n",
    "# Pretrain the PPO2 model\n",
    "model.pretrain(dataset, n_epochs=5000)\n",
    "\n",
    "# As an option, you can train the RL agent\n",
    "# model.learn(int(1e5))\n",
    "\n",
    "# Test the pre-trained model\n",
    "env = model.get_env()\n",
    "obs = env.reset()\n",
    "\n",
    "reward_sum = 0.0\n",
    "for _ in range(1000):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "        env.render()\n",
    "        if done:\n",
    "                print(reward_sum)\n",
    "                reward_sum = 0.0\n",
    "                obs = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-narrow",
   "metadata": {},
   "source": [
    "## Stable Baselines Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-turtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines import DQN\n",
    "from stable_baselines.gail import generate_expert_traj\n",
    "\n",
    "model = DQN('MlpPolicy', 'CartPole-v1', verbose=1)\n",
    "      # Train a DQN agent for 1e5 timesteps and generate 10 trajectories\n",
    "      # data will be saved in a numpy archive named `expert_cartpole.npz`\n",
    "generate_expert_traj(model, 'expert_cartpole', n_timesteps=int(1e5), n_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines import PPO2\n",
    "from stable_baselines.gail import ExpertDataset\n",
    "# Using only one expert trajectory\n",
    "# you can specify `traj_limitation=-1` for using the whole dataset\n",
    "dataset = ExpertDataset(expert_path='expert_cartpole.npz',\n",
    "                        traj_limitation=1, batch_size=128)\n",
    "\n",
    "model = PPO2('MlpPolicy', 'CartPole-v1', verbose=1)\n",
    "# Pretrain the PPO2 model\n",
    "model.pretrain(dataset, n_epochs=1000)\n",
    "\n",
    "# As an option, you can train the RL agent\n",
    "# model.learn(int(1e5))\n",
    "\n",
    "# Test the pre-trained model\n",
    "env = model.get_env()\n",
    "obs = env.reset()\n",
    "\n",
    "reward_sum = 0.0\n",
    "for _ in range(1000):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "        env.render()\n",
    "        if done:\n",
    "                print(reward_sum)\n",
    "                reward_sum = 0.0\n",
    "                obs = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones(3)*np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-march",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
