{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "classical-comedy",
   "metadata": {},
   "source": [
    "## Define ORCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "structured-steering",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from crowd_sim.envs.policy.orca import ORCA\n",
    "from crowd_sim.envs.utils.state import JointState\n",
    "\n",
    "class Suicide(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class ORCAPolicy(object):\n",
    "    def __init__(self, suicide_if_stuck=False):\n",
    "        self.simulator = ORCA()\n",
    "        self.suicide_if_stuck = suicide_if_stuck\n",
    "\n",
    "    def reset(self):\n",
    "        self.simulator.reset()\n",
    "\n",
    "    def predict(self, obs, env):\n",
    "        self.simulator.time_step = env._get_dt()\n",
    "        other_agent_states = [\n",
    "            agent.get_observable_state() for agent in env.soadrl_sim.humans + env.soadrl_sim.other_robots]\n",
    "        action = self.simulator.predict(\n",
    "            JointState(env.soadrl_sim.robot.get_full_state(), other_agent_states),\n",
    "            env.soadrl_sim.obstacle_vertices,\n",
    "            env.soadrl_sim.robot,\n",
    "        )\n",
    "        if self.suicide_if_stuck:\n",
    "            if action.v < 0.1:\n",
    "                return Suicide()\n",
    "        vx = action.v * np.cos(action.r)\n",
    "        vy = action.v * np.sin(action.r)\n",
    "        return np.array([vx, vy, 0.1*(np.random.random()-0.5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-vaccine",
   "metadata": {},
   "source": [
    "## Setup Env and Dummy Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "designed-insider",
   "metadata": {},
   "outputs": [],
   "source": [
    "from navrep.envs.navreptrainenv import NavRepTrainEnv\n",
    "env = NavRepTrainEnv(silent=True, scenario='train', adaptive=False, collect_statistics=False)\n",
    "env.soadrl_sim.human_num = 20\n",
    "\n",
    "policy=ORCAPolicy(suicide_if_stuck=True)\n",
    "def policy_wrapper(_obs):\n",
    "    return policy.predict(_obs, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-athletics",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "worldwide-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines import DQN\n",
    "from stable_baselines.gail import generate_expert_traj\n",
    "from crowd_sim.envs.policy.orca import ORCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "qualified-clinic",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Observation space type not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-61338f33d137>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#generate_expert_traj(model, 'expert_cartpole', n_timesteps=int(1e5), n_episodes=10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mgenerate_expert_traj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'orca_1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/stable_baselines/gail/dataset/record_expert.py\u001b[0m in \u001b[0;36mgenerate_expert_traj\u001b[0;34m(model, save_path, env, n_timesteps, n_episodes, image_folder)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Sanity check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     assert (isinstance(env.observation_space, spaces.Box) or\n\u001b[0;32m---> 51\u001b[0;31m             isinstance(env.observation_space, spaces.Discrete)), \"Observation space type not supported\"\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     assert (isinstance(env.action_space, spaces.Box) or\n",
      "\u001b[0;31mAssertionError\u001b[0m: Observation space type not supported"
     ]
    }
   ],
   "source": [
    "#model = DQN('MlpPolicy', 'CartPole-v1', verbose=1)\n",
    "      # Train a DQN agent for 1e5 timesteps and generate 10 trajectories\n",
    "      # data will be saved in a numpy archive named `expert_cartpole.npz`\n",
    "#generate_expert_traj(model, 'expert_cartpole', n_timesteps=int(1e5), n_episodes=10)\n",
    "\n",
    "generate_expert_traj(policy_wrapper, 'orca_1', env, n_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-exclusion",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "median-compatibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions (5000, 1)\n",
      "obs (5000, 4)\n",
      "rewards (5000,)\n",
      "episode_returns (10,)\n",
      "episode_starts (5000,)\n",
      "Total trajectories: 1\n",
      "Total transitions: 998\n",
      "Average returns: 500.0\n",
      "Std for returns: 0.0\n",
      "Creating environment from the given name, wrapped in a DummyVecEnv.\n",
      "WARNING:tensorflow:From /home/mads/miniconda3/envs/NavRepEnv/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Pretraining with Behavior Cloning...\n",
      "==== Training progress 10.00% ====\n",
      "Epoch 100\n",
      "Training loss: 0.591516, Validation loss: 0.577229\n",
      "\n",
      "==== Training progress 20.00% ====\n",
      "Epoch 200\n",
      "Training loss: 0.566989, Validation loss: 0.551381\n",
      "\n",
      "==== Training progress 30.00% ====\n",
      "Epoch 300\n",
      "Training loss: 0.528652, Validation loss: 0.519976\n",
      "\n",
      "==== Training progress 40.00% ====\n",
      "Epoch 400\n",
      "Training loss: 0.500740, Validation loss: 0.481484\n",
      "\n",
      "==== Training progress 50.00% ====\n",
      "Epoch 500\n",
      "Training loss: 0.462543, Validation loss: 0.430930\n",
      "\n",
      "==== Training progress 60.00% ====\n",
      "Epoch 600\n",
      "Training loss: 0.423922, Validation loss: 0.422883\n",
      "\n",
      "==== Training progress 70.00% ====\n",
      "Epoch 700\n",
      "Training loss: 0.406490, Validation loss: 0.396774\n",
      "\n",
      "==== Training progress 80.00% ====\n",
      "Epoch 800\n",
      "Training loss: 0.375151, Validation loss: 0.402095\n",
      "\n",
      "==== Training progress 90.00% ====\n",
      "Epoch 900\n",
      "Training loss: 0.360384, Validation loss: 0.340232\n",
      "\n",
      "==== Training progress 100.00% ====\n",
      "Epoch 1000\n",
      "Training loss: 0.342948, Validation loss: 0.356529\n",
      "\n",
      "Pretraining done.\n",
      "[405.]\n",
      "[243.]\n",
      "[225.]\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines import PPO2\n",
    "from stable_baselines.gail import ExpertDataset\n",
    "# Using only one expert trajectory\n",
    "# you can specify `traj_limitation=-1` for using the whole dataset\n",
    "dataset = ExpertDataset(expert_path='expert_cartpole.npz',\n",
    "                        traj_limitation=1, batch_size=128)\n",
    "\n",
    "model = PPO2('MlpPolicy', 'CartPole-v1', verbose=1)\n",
    "# Pretrain the PPO2 model\n",
    "model.pretrain(dataset, n_epochs=1000)\n",
    "\n",
    "# As an option, you can train the RL agent\n",
    "# model.learn(int(1e5))\n",
    "\n",
    "# Test the pre-trained model\n",
    "env = model.get_env()\n",
    "obs = env.reset()\n",
    "\n",
    "reward_sum = 0.0\n",
    "for _ in range(1000):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "        env.render()\n",
    "        if done:\n",
    "                print(reward_sum)\n",
    "                reward_sum = 0.0\n",
    "                obs = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "soviet-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/mads/navrep/datasets/V/navreptrain/099_scans_robotstates_actions_rewards_dones.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adjacent-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "italic-routine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions\n",
      "obs\n",
      "rewards\n",
      "episode_returns\n",
      "episode_starts\n"
     ]
    }
   ],
   "source": [
    "tmp = np.load(\"expert_cartpole.npz\")\n",
    "for k in tmp.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "contained-portable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scans\n",
      "robotstates\n",
      "actions\n",
      "rewards\n",
      "dones\n"
     ]
    }
   ],
   "source": [
    "tmp = np.load(path)\n",
    "for k in tmp.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-fleet",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
