{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cosmetic-mileage",
   "metadata": {},
   "source": [
    "## ORCA Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-sound",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from crowd_sim.envs.policy.orca import ORCA\n",
    "from crowd_sim.envs.utils.state import JointState\n",
    "\n",
    "class Suicide(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "class ORCAPolicy(object):\n",
    "    def __init__(self, suicide_if_stuck=False):\n",
    "        self.simulator = ORCA()\n",
    "        self.suicide_if_stuck = suicide_if_stuck\n",
    "\n",
    "    def reset(self):\n",
    "        self.simulator.reset()\n",
    "\n",
    "    def predict(self, obs, env):\n",
    "        self.simulator.time_step = env._get_dt()\n",
    "        other_agent_states = [\n",
    "            agent.get_observable_state() for agent in env.soadrl_sim.humans + env.soadrl_sim.other_robots]\n",
    "        action = self.simulator.predict(\n",
    "            JointState(env.soadrl_sim.robot.get_full_state(), other_agent_states),\n",
    "            env.soadrl_sim.obstacle_vertices,\n",
    "            env.soadrl_sim.robot,\n",
    "        )\n",
    "        if self.suicide_if_stuck:\n",
    "            if action.v < 0.1:\n",
    "                return Suicide()\n",
    "        vx = action.v * np.cos(action.r)\n",
    "        vy = action.v * np.sin(action.r)\n",
    "        return np.array([vx, vy, 0.1*(np.random.random()-0.5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-pottery",
   "metadata": {},
   "source": [
    "## Play Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-roommate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def play_policy(env, n_sequences, episode_length=1000,\n",
    "                         subset_index=0, n_subsets=1,\n",
    "                         render=True,\n",
    "                         policy=ORCAPolicy(),\n",
    "                         archive_dir=os.path.expanduser(\"~/navrep/datasets/V/ian\")\n",
    "                         ):\n",
    "    \"\"\"\n",
    "    if n_subsets is None, the whole set of sequences is generated (n_sequences)\n",
    "    if n_subsets is a number > 1, this function only generates a portion of the sequences\n",
    "    \"\"\"\n",
    "    indices = np.arange(n_sequences)\n",
    "    if n_subsets > 1:  # when multiprocessing\n",
    "        indices = np.array_split(indices, n_subsets)[subset_index]\n",
    "    for n in indices:\n",
    "        scans = []\n",
    "        robotstates = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "        policy.reset()\n",
    "        obs = env.reset()\n",
    "        for i in range(episode_length):\n",
    "            # step\n",
    "            action = policy.predict(obs, env)\n",
    "            if isinstance(action, Suicide):\n",
    "                obs = env.reset()\n",
    "                rew = 0\n",
    "                action = np.array([0, 0, 0])\n",
    "                done = True\n",
    "            else:\n",
    "                obs, rew, done, _ = env.step(action)\n",
    "            scans.append(obs[0])\n",
    "            robotstates.append(obs[1])\n",
    "            actions.append(action)\n",
    "            rewards.append(rew)\n",
    "            dones.append(done)\n",
    "            if render:\n",
    "                env.render()\n",
    "            if done:\n",
    "                policy.reset()\n",
    "                obs = env.reset()\n",
    "        dones[-1] = True\n",
    "\n",
    "        scans = np.array(scans)\n",
    "        robotstates = np.array(robotstates)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards)\n",
    "        dones = np.array(dones)\n",
    "        data = dict(scans=scans, robotstates=robotstates, actions=actions, rewards=rewards, dones=dones)\n",
    "        #if archive_dir is not None:\n",
    "         #   make_dir_if_not_exists(archive_dir)\n",
    "          #  archive_path = os.path.join(\n",
    "           #     archive_dir, \"{:03}_scans_robotstates_actions_rewards_dones.npz\".format(n)\n",
    "            #)\n",
    "            #np.savez_compressed(archive_path, **data)\n",
    "            #print(archive_path, \"written.\")\n",
    "    env.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "from navrep.envs.e2eenv import E2E1DNavRepEnv\n",
    "env = E2E1DNavRepEnv(silent=True, scenario='train', adaptive=False, collect_statistics=False)\n",
    "env.soadrl_sim.human_num = 2\n",
    "play_policy(env,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-adams",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
